---
title: "Machine Learning-Driven Predictive Modeling for Opioid Use Disorder in Chronic Pain Patients on Long-Term Opioid Use"
format: html
editor: visual
embed-resources: true
---

## Overview {#sec-overview}

This predictive model aims to identify opioid use disorder among chronic pain patients on long-term opioid therapy by incorporating diverse risk factors, grounded in the biopsychosocial model of pain. It provides a robust predictive framework and serves as a foundation for more advanced machine learning models that will incorporate a broader range of psychiatric conditions, ultimately enhancing early identification and intervention strategies for at-risk patients. For this project, I am collaborating with two psychology experts specializing in substance use disorders, including opioid use disorder.

-   Link to final project GitHub repository) : https://github.com/yoonjae-hub/BMIN503_Final_Project.git

## 1. Introduction

Nearly 51.6 millions of Americans living with chronic pain (Rikard, Strahan et al. 2023). Managing chronic non-cancer pain (CNCP) is particularly challenging secondary to its complex neurobiological and psychosocial mechanisms (Fillingim, Bruehl et al. 2014), profoundly impacting patients’ physical, psychological, and social well-being (Genova, Dix et al. 2020). CNCP is defined as persistent or recurrent pain lasting three months or longer, and encompasses a range of non-malignant conditions, including neuropathic pain, rheumatoid arthritis, lower back pain, osteoarthritis, fibromyalgia, and various other conditions. A major concern with CNCP is the ongoing use of opioids. Opioids are widely prescribed for chronic pain, but can cause adverse outcomes, such as unintentional overdose, misuse, addiction, and even death (Dahlhamer, Connor et al. 2021, Dowell, Ragan et al. 2022). As many as 25% of patients treated with opioids for chronic pain are known to develop opioid use disorder (OUD) (Cordier Scott and Lewis 2016).

Leveraging AI in healthcare can transform OUD management by analyzing large datasets to develop machine learning (ML) algorithms that identify individuals at risk. This approach enables earlier intervention and more personalized care. Recently, the FDA approved AvertD, an ML-based risk score model designed to assess genetic predisposition to OUD using a prescription-only genotyping test (U.S. Food and Drug Administration 2024). However, AvertD relies solely on buccal swab specimens, excluding significant socioeconomic, physical, and psychological factors associated with OUD. Additionally, AvertD is designed for first-time opioid users, while OUD incidence is higher among individuals on long-term opioid therapy. This highlights the need for an ML model that targets chronic opioid users and integrates broader risk factors for better accessibility and applicability. Therefore, this study aims to develop an ML model to predict OUD among chronic pain patients who have been on opioid therapy, integrating biopsychosocial factors that are known to influence OUD risk.

This problem is inherently interdisciplinary because pain, particularly in the context of chronic pain and opioid use disorder (OUD), cannot be fully understood or addressed from a single disciplinary perspective. The biopsychosocial model emphasizes that pain is influenced not only by biological factors but also by psychological and social dimensions. Therefore, incorporating experts from psychology is essential to explore the psychological underpinnings of pain, such as emotional distress, coping mechanisms, and the role of comorbid psychiatric conditions like anxiety, depression, or substance use disorders.

## 2. Methods {#sec-methods}

**Data source** This project used dataset that were collected for a larger parent study that evaluated the phenotypic and genotypic profiles of CNCP patients on opioid therapy who did and did not develop an OUD. Data were collected between November 2012 and September 2018. This study includes 1,356 patients who were receiving long-term (6 months or more) opioid therapy (LTOT) to manage CNCP. Subjects were collected from three different regions (Pennsylvania, Washington, and Utah) in the United States. The eligibility criteria for the parent study were: individuals who were (1) aged 18 or older, (2) Caucasian and of European descent (defined as 3 or 4 grandparents were European), (3) experiencing CNCP of musculoskeletal or neuropathic origin persisting for at least 6 months, (4) having no history of substance abuse (except nicotine) before starting LTOT, and (5) having received LTOT to treat their pain for at least the past 6 months. Due to the genotypic and phenotypic objectives of the parent study, persons who are non-Caucasian or with a previous history of substance use disorder were excluded from the study. Additional criteria for exclusion included severe psychiatric conditions that hindered the ability to provide informed consent or complete the questionnaire; and individuals experiencing pain conditions not arising from musculoskeletal/neuropathic origin (e.g., cancer, gynecologic issues, abdominal discomfort, visceral concerns, dental problems, neuropathic pain due to metabolic disease).

**Determination of OUD** For this analysis, participants were grouped as either ‘cases’ or ‘controls’ based on whether or not they developed OUD after initiating LTOT. The control group included patients who did not have evidence of opioid abuse or meet the criteria for OUD at the time of assessment. Their electronic health records (EHR) were reviewed monthly for 12 consecutive months after enrolling in the study to ensure they did not develop an OUD after completing the baseline assessment. To be considered controls, patients receiving LTOT needed to have negative urine drug screens (which detect opioid metabolites and other illicit drugs), have no record of current or past SUD, and have evidence of no aberrant drug-related behaviors assessed using an expert developed checklist (Cheatle, O'Brien et al. 2013). Cases were patients who did not have a history of SUD (except nicotine) prior to beginning LTOT and currently met DSM-IV criteria for ‘opioid dependence’, which were obtained at the time of enrolling in the study.

**Feature Selection** Given the biopsychosocial aspects of OUD, the initial analysis will explore variables such as demographic factors (age, sex, race, ethnicity, marital status, education, employment, financial situation), pain (severity, interference), nicotine dependence, psychiatric conditions (depression, anxiety, pain catastrophizing, mental defeat, suicidality), and social support. To check multicollinearity among these variables, variance inflation factor (VIF) was used.

**Implementation of ML** This study will use a supervised learning approach to develop the predictive model. Given the binary outcome, algorithms such as logistic regression, random forest, and gradient boosting will be employed. To compare performance, each algorithm will be tested using area under the receiver operating characteristic curve (ROC-AOC). A potential challenge in this approach is missing data, as some variables rely on total scores. Missing data in these variables can lead to more extensive data gaps since individual items are not considered separately. To address this, missing data imputation techniques will be considered. Also, to ensure model generalization and detect potential overfitting/underfitting, cross-validation methods (k-fold cross-validation) will be applied.

## 2.1 Loading Packages

```{r}
library(tidyverse) 
library(ggplot2) # For graphs
library(lubridate) # For manipulating dates
library(gtsummary) #Summary statistics
library(RColorBrewer) # For coloring the plots
library(cowplot) # For combining multiple plots into one
library(ggpubr) # Combining multiple graphs into one figure 
library(pROC) # For cross-validation
library(dplyr) # For data cleaning
library(gtsummary)
library(haven)
library(rsample)
library(car)
library(tidymodels)
library(dotwhisker)
library(parsnip)
```

## 2.2 Variables

```{r}
#Predictors
#Demographic - gender (DEM002), age (DEM003), marital status (DEM004), living along (DEM005), education (DEM006), employment (DEM007), financial status (DEM008)
#Plesae note that due to the genotypic and phenotypic objectives of the parent study, persones who are non-Caucasian or with a previous history of substance use disorder were excluded from the study)

demp_v2 <- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/demp_v2.sav")

#Pain - severity, interference - Brief Pain Inventory
bpip <- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/bpip.sav")

#Smoking- Fagerstrom scale
fnts<- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/fnts.sav")

#Alcohol
aadidm <- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/aadidm.sav")

#Anxiety/Depression-PHQ-4
phq4 <-read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/phq4.sav")

#Pain catastrophizing (Subscale) - Coping strategies questionnaire
copsq<- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/copsq.sav")

#Mental defeat-Pain perception scale
pps <-read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/pps.sav")

#Suicidality - SBQ-R
sbqr <- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/sbqr.sav")

#Social support - Duke social support index
dssi_v2 <- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/dssi_v2.sav")

#Outcome - Opioid Use Disorder
pevc <- read_sav("/Users/yoonjaelee/Desktop/a_Upenn_PhD_2023-2026/2nd year_Fall 2024/EHR class/Final Project/R01 Data/pevc.sav")

```

## 2.3 Cleaning and Transforming Data for Analysis

```{r}

#Demographic - gender, age, marital status, living status, education, employment, financial status
demo <-demp_v2 %>%
  select(DEM002, DEM003, DEM004, DEM005, DEM006, DEM007, DEM008, SubjID)
demo <- demo %>%
  mutate(across(
    where(~inherits(., "haven_labelled")), 
    as_factor
  ))
demo <- demo %>%
  mutate(across(
    where(~inherits(., "haven_labelled") & is.numeric(.)),
    as.numeric
  ))

demo$DEM003[which(demo$DEM003 == "(X)Subject refused to answer")] <- NA
demo$DEM005[which(demo$DEM005 == "Do not know/refused")] <- NA
demo$DEM005 <- droplevels(demo$DEM005)
levels(demo$DEM005)
demo$DEM006[which(demo$DEM006 == "(X)Subject refused to answer")] <- NA
demo$DEM006 <- droplevels(demo$DEM006)
levels(demo$DEM006)
demo$DEM007 <- as.character(demo$DEM007)
demo$DEM007[which(demo$DEM007 == "Do not know/refused")] <- NA
demo$DEM007 <- as_factor(demo$DEM007)
demo$DEM007 <- droplevels(demo$DEM007)
levels(demo$DEM007)
demo$DEM008 <- as.character(demo$DEM008)
demo$DEM008[which(demo$DEM008 == "Do not know/Refused")] <- NA
demo$DEM008 <- as_factor(demo$DEM008)
demo$DEM008 <- droplevels(demo$DEM008)
levels(demo$DEM008)
#transforming to factor (all, except DEM003 - age)
demo$DEM002 <- as_factor(demo$DEM002)
demo$DEM003 <- as.numeric(demo$DEM003)
demo$DEM004 <- as_factor(demo$DEM004)
demo$DEM005 <- as_factor(demo$DEM005)
demo$DEM006 <- as_factor(demo$DEM006)
demo$DEM007 <- as_factor(demo$DEM007)
demo$DEM008 <- as_factor(demo$DEM008)


#checking (examples)
class(demo$DEM004)       # Should show "factor"
levels(demo$DEM005)      # no 'do not know'refused'
is.numeric(demo$DEM005)  # Should return FALSE

#pain
#pain-severity
pain_severity <- bpip %>%
  filter(month == 0) %>%
  mutate(pain_severity_sum = if_else(if_any(c(BPI001, BPI002, BPI003, BPI004), ~ . == -9),
                                     NA_real_,
                                     BPI001 + BPI002 + BPI003 + BPI004)) %>%
  select(pain_severity_sum, SubjID)

#pain-interference
pain_interf <- bpip %>%
  filter(month == 0) %>%
  mutate(pain_interf_sum = if_else(if_any(c(BPI007A, BPI007B, BPI007C, BPI007D, BPI007E, BPI007G), ~ . == -9),
                                   NA_real_, # changing -9 to NA value
                                   BPI007A + BPI007B + BPI007C + BPI007D + BPI007E + BPI007G)) %>%
  select(pain_interf_sum, SubjID)

#smoking : conditional survey (if FNTS000 entry question is 0, rest of questions are a system missing value; No missing value in FNTS000)
missing_fnts <- sum(is.na(fnts$FNTS000))
missing_fnts #for the initial screening question, there is no missing data.

# 001, 004: Multiple-choice items (0-3) / # 2,3,5,6: Yes/No items (0-1)
fnts_cleaned <- fnts %>%
  mutate(
    fnts_sum = ifelse(FNTS000 == 0, NA,
                         FNTS001 + FNTS004 + 
                         FNTS002 + FNTS003 + FNTS005 + FNTS006)) %>%
  replace_na(list(fnts_sum=0))

fnts_cleaned <- fnts_cleaned %>%
  filter(month==0)

fnts_cleaned <- fnts_cleaned %>%
  mutate(dependence_level = case_when(
    fnts_sum == 0 ~ "Non-smoker",
    fnts_sum >= 1 & fnts_sum <= 2 ~ "Low",
    fnts_sum >= 3 & fnts_sum <= 4 ~ "Low to Moderate",
    fnts_sum >= 5 & fnts_sum <= 7 ~ "Moderate",
    fnts_sum >= 8 ~ "High")) %>%
  select(SubjID, fnts_sum, dependence_level)

fnts_cleaned_2 <- fnts_cleaned %>%
  select(SubjID, dependence_level) #sort out only two variables

#Alcohol use - more conservative, in the last 3 months
aadidm_cleaned <- aadidm %>%
  filter(month == 0) %>%
  mutate(
    alcohol = ifelse(AAD001 == 1, 1, 0),
    alcohol = replace(alcohol, is.na(alcohol), 0)  # Replace NA with 0
  ) %>%
  select(SubjID, alcohol)
table(aadidm_cleaned$alcohol)

#PHQ-4 (Anxiety/Depression)
phq4 <- phq4 %>%
  filter (month == 0)
#PHQ-4: dichotomizing Anxiety, cutoff >=3
#PHQ-4: dichotomizing Depression, cutoff >=3
phq4 <- phq4 %>%
  mutate(Anxiety = ifelse(PHQ001 + PHQ002 >= 3, 1, 0),
         Depression = ifelse(PHQ003 + PHQ004 >= 3, 1, 0))
phq4_cleaned <- phq4 %>%
  select(Anxiety, Depression, SubjID)

phq4_cleaned$Anxiety <- as.factor(phq4_cleaned$Anxiety)
phq4_cleaned$Depression <- as.factor(phq4_cleaned$Depression)

#CSQ - Pain catastrophizing subscale only (score already calculated in the orignal dataset: CATAT)
ca_catastro <- copsq %>%
  filter(month == 0) %>%
  mutate(ca_sum = if_else(if_any(c(COP005, COP012, COP014, COP028, COP038, COP042), ~ . == -9),
                                   NA_real_,
                                   COP005 + COP012 + COP014 + COP028 + COP038 + COP042)) %>%
  select(ca_sum,SubjID)

##PSPS -HAVE some missing vales (pps_sum)
pps_cleaned <-pps %>%
  mutate(pps_sum= rowSums(across(starts_with("PPS"), ~ ifelse(. == -9, NA, .), .names = "new_{.col}")), na.rm= FALSE)
pps_cleaned <- pps_cleaned%>%
  select(pps_sum, SubjID)

missing_pps <- sum(is.na(pps_cleaned$pps_sum))
missing_pps #n=85

#Suicidality: cutoff >=8 (higher risk) for clinical samples / no missing values, but '-9' refused to answer-> Total score is NA whenever a -9 value is present in any of the specified columns. (NA in total score = only 6 data)
#scoring website: https://www.ncbi.nlm.nih.gov/books/NBK571017/box/ch3.b11/?report=objectonly

sbqr_cleaned <- sbqr %>%
  filter(month==0) %>%
  mutate(
    SBQ001 = case_when(
      SBQ001 %in% c(3, 4) ~ 3, #regrouping
      SBQ001 %in% c(5, 6) ~ 4,
      TRUE ~ SBQ001),
    SBQ003 = case_when(
      SBQ003 %in% c(2, 3) ~ 2,
      SBQ003 %in% c(4, 5) ~ 3,
      TRUE ~ SBQ003))

sbqr_cleaned <- sbqr_cleaned %>%
     mutate(sbqr_sum = if_else(
      rowSums(select(., SBQ001, SBQ002, SBQ003, SBQ004) == -9) > 0,
      NA_real_,
      rowSums(select(., SBQ001, SBQ002, SBQ003, SBQ004), na.rm = TRUE))) %>%
  select(SubjID, sbqr_sum)

#Categorize suicidality risk based on sbqr_sum (>=8; high risk, <8: low risk, NA=> NA)(>=8; high risk, <8: low risk, NA=> NA); missing data = 6ea
sbqr_cleaned <- sbqr_cleaned %>%
  mutate(
    suicidality_risk = case_when(
      sbqr_sum >= 8 ~ "High Risk",
      sbqr_sum < 8 ~ "Low Risk",
      is.na(sbqr_sum) ~ NA_character_))

suicidality <- sbqr_cleaned %>%
  select(SubjID, suicidality_risk)

#social support (DSSI) : 3 subscales scores
dssi_cleaned <- dssi_v2 %>%
  filter(month==0) %>%
  select(SubjID, SIDSSI, SSDSSI, ISDSSI)
#social interaction (SIS, 4 item) : assessing the amount of time: 3 point likert scale (missing data: NONE)
## dssi_cleaned$SIDSSI

#subjective social support (SSS, 7 items): assessing depth or quality of relationship: 3point (missing data: 14)
## dssi_cleaned$SSDSSI

#Instrumental social support (ISS, 12 items): degree to which others can be counted upon with daily activities;0/1; (missing data: 4)
## dssi_cleaned$ISDSSI

#outcome: opioid use disorder (cohort - 1: case=OUD, 0: control=no OUD)
cohort <- read.csv("/Users/yoonjaelee/Desktop/Dr.CHEATLE ML PROJECT/pevc_compiled.csv")
cohort_cleaned <- cohort %>%
  select(SubjID,cohort)
cohort_cleaned$cohort <- as.factor(cohort_cleaned$cohort) #changing the class to categorical values.
class(cohort_cleaned$cohort) #factor
#removing 16 Nas
cohort_cleaned <- cohort_cleaned[!is.na(cohort_cleaned$cohort), ]
cohort_cleaned <- cohort_cleaned %>%
  filter(SubjID != 0) #remove SubjID==0
table(cohort_cleaned$cohort)  #cohort 0:1042; 1:486

##Merging data_using left_join
merging1 <- left_join(cohort_cleaned, demo, by = "SubjID")
## Note that when merging demo file with cohort_cleaned file, there are missing data -> remove; not registered for the study. (removed 173 patients)
merging1 <- merging1 %>%
  drop_na()
merging2 <- left_join(merging1, pain_severity, by = "SubjID")
merging3 <- left_join(merging2, pain_interf, by = "SubjID")
merging4 <- left_join(merging3, fnts_cleaned_2, by = "SubjID")
merging5 <- left_join(merging4, phq4_cleaned, by = "SubjID")
merging6 <- left_join(merging5, ca_catastro, by = "SubjID")
merging7 <- left_join(merging6, pps_cleaned, by = "SubjID")
merging8 <- left_join(merging7, suicidality, by = "SubjID")
merging9 <- left_join(merging8, aadidm_cleaned, by = "SubjID")
final_data <- left_join(merging9, dssi_cleaned, by = "SubjID") #final sample N= 1331
table(final_data$cohort) #901/430

```

## 2.4 Exploratory Data Analysis (Distribution/Association)

Based on distribution analysis, most of variables were skewed. However, given the large sample size, we can rely on the Central Limit Theorem to approximate normalilty.

```{r}
#Categorical variables
#1-1) Fnts - Visualization
fnts_eda <- left_join(cohort_cleaned, fnts_cleaned, by = "SubjID") %>%
  filter(!is.na(dependence_level), !is.na(cohort))

ggplot(fnts_eda, aes(x = dependence_level, fill = as.factor(cohort))) +
  geom_bar(position = "dodge") +
  labs(x = "Dependence Level", y = "Count", fill = "OUD Status") +
  scale_fill_discrete(labels = c("Control", "Case")) +
  theme_minimal()

oud_smoking <- table(final_data$cohort, final_data$dependence_level)
print(oud_smoking) # All cells >5. okay to perform chi-square test

chi_oud_smoking <- chisq.test(oud_smoking)
print(chi_oud_smoking) # p-value < 2.2e-16 -> Significantly associated.

#2-1) phq4 - Distribution
phq4_eda <- left_join(cohort_cleaned, phq4_cleaned, by = "SubjID") %>%
  filter(!is.na(Anxiety), !is.na(Depression))

ggplot(phq4_eda, aes(x = Anxiety, fill = as.factor(cohort))) +
    geom_bar(position = "dodge") +
    labs(x = "Anxiety", y = "Count", fill = "OUD Status") +
   scale_fill_discrete(labels = c("Control", "Case")) +
   scale_x_discrete(labels = c("0" = "No", "1" = "Yes")) +
    theme_minimal()

ggplot(phq4_eda, aes(x = Depression, fill = as.factor(cohort))) +
    geom_bar(position = "dodge") +
    labs(x = "Depression", y = "Count", fill = "OUD Status") +
   scale_fill_discrete(labels = c("Control", "Case")) +
   scale_x_discrete(labels = c("0" = "No", "1" = "Yes")) +
    theme_minimal()

#2-2) phq4 - Association
oud_anxiety <- table(final_data$cohort, final_data$Anxiety)
print(oud_anxiety) # expected counts >5. okay to perform chi-square test
chi_oud_anxiety <- chisq.test(oud_anxiety, correct=F)
print(chi_oud_anxiety) # p-value = 2.391e-09 -> Significantly associated.

oud_dep <- table(final_data$cohort, final_data$Depression)
print(oud_dep) # expected counts >5. okay to perform chi-square test
chi_oud_dep <- chisq.test(oud_dep, correct=F)
print(chi_oud_dep) # p-value = 3.518e-11 -> Significantly associated.

#3-1) SBQR-R - Distribution
sbqr_eda <- left_join(cohort_cleaned, sbqr_cleaned, by = "SubjID")%>%
  filter(!is.na(suicidality_risk))

ggplot(sbqr_eda, aes(x = suicidality_risk, fill = as.factor(cohort))) +
    geom_bar(position = "dodge") +
    labs(x = "Suicidality risk", y = "Count", fill = "OUD Status") +
   scale_fill_discrete(labels = c("Control", "Case")) +
   scale_x_discrete(labels = c("0" = "No", "1" = "Yes")) +
    theme_minimal()
    
#3-2) SBQR-R - Association
oud_sui <- table(final_data$cohort, final_data$suicidality_risk)
print(oud_sui) # expected counts >5. okay to perform chi-square test
chi_oud_sui <- chisq.test(oud_sui, correct=F)
print(chi_oud_sui) # p-value = 3.123e-11 -> Significantly associated.


#Continuous variables 
#1-1)Pain severity - Visualization
ps_data_noNA <- final_data %>%
  filter(!is.na(cohort), !is.na(pain_severity_sum))

ggplot(ps_data_noNA, aes(x = as.factor(cohort), y = pain_severity_sum, fill = as.factor(cohort))) +
    geom_violin(fill = "lightyellow", draw_quantiles = c(0.25, 0.5, 0.75)) +
    labs(x = "Cohort", y = "Pain Severity Sum", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
   geom_jitter(height = 0, width = 0.1) +
    theme_minimal()

ggplot(ps_data_noNA, aes(x = pain_severity_sum, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "Pain Severity Sum", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()
    
#1-2)Pain severity - Association
ps.nonoud <- ps_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,pain_severity_sum)

ps.oud <- ps_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, pain_severity_sum)

var.test(ps.nonoud$pain_severity_sum, ps.oud$pain_severity_sum) #p-value = 0.001172 -> unequal variance -> welch's t-test
ps_ttest <- t.test(ps.nonoud$pain_severity_sum, ps.oud$pain_severity_sum)
print(ps_ttest) #statistically different.

#2-1)pain interference - Visualization
pi_data_noNA <- final_data %>%
  filter(!is.na(cohort), !is.na(pain_interf_sum))


ggplot(pi_data_noNA, aes(x = as.factor(cohort), y = pain_interf_sum, fill = as.factor(cohort))) +
    geom_violin(fill = "lightyellow", draw_quantiles = c(0.25, 0.5, 0.75)) +
    labs(x = "Cohort", y = "Pain Interference Sum", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
   geom_jitter(height = 0, width = 0.1) +
    theme_minimal()

ggplot(pi_data_noNA, aes(x = pain_interf_sum, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "Pain Interference Sum", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()

#2-2)pain interference - Association
pi.nonoud <- pi_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,pain_interf_sum)

pi.oud <- pi_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, pain_interf_sum)

var.test(pi.nonoud$pain_interf_sum, pi.oud$pain_interf_sum) # p-value = 0.8104 -> equal variance
pi_ttest <- t.test(pi.nonoud$pain_interf_sum, pi.oud$pain_interf_sum, var.equal = T)
print(pi_ttest) #statistically different.

#3-1) pain catastrophizing - Distribution
pc_data_noNA <- final_data %>%
  filter(!is.na(cohort), !is.na(ca_sum))

ggplot(pc_data_noNA, aes(x = as.factor(cohort), y = ca_sum, fill = as.factor(cohort))) +
    geom_violin(fill = "lightyellow", draw_quantiles = c(0.25, 0.5, 0.75)) +
    labs(x = "Cohort", y = "Pain Catastrophizing Sum", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
   geom_jitter(height = 0, width = 0.1) +
    theme_minimal()

ggplot(pc_data_noNA, aes(x = ca_sum, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "Pain Catastrohpizing", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()
    
#3-2) pain catastrophizing - Association
pc.nonoud <- pc_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,ca_sum)

pc.oud <- pc_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, ca_sum)

var.test(pc.nonoud$ca_sum, pc.oud$ca_sum) #p-value = 0.01178 -> unequal variance -> welch's t-test
pc_ttest <- t.test(pc.nonoud$ca_sum, pc.oud$ca_sum)
print(pc_ttest) #statistically different.

#4-1)PSPS - Mental defeat - Distribution
psps_data_noNA <- final_data %>%
  filter(!is.na(pps_sum))

ggplot(psps_data_noNA, aes(x = as.factor(cohort), y = pps_sum, fill = as.factor(cohort))) +
    geom_violin(fill = "lightyellow", draw_quantiles = c(0.25, 0.5, 0.75)) +
    labs(x = "Cohort", y = "Mental defeat", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
   geom_jitter(height = 0, width = 0.1) +
    theme_minimal()

ggplot(psps_data_noNA, aes(x = ca_sum, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "Mental Defeat", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()
    
#4-2) PSPS - Mental defeat - Association
md.nonoud <- psps_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,pps_sum)

md.oud <- psps_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, pps_sum)

var.test(md.nonoud$pps_sum, md.oud$pps_sum) #p-value = 0.3597 -> equal variance -> Levene's t-test
md_ttest <- t.test(md.nonoud$pps_sum, md.oud$pps_sum, var.equal=T)
print(md_ttest) #statistically different.


#5-1) DSSI - Distribution (SIDSSI, SSDSSI, ISDSSI (all continuous))
dssi_data_noNA <- final_data %>%
  filter(!is.na(SIDSSI), !is.na(SSDSSI), !is.na(ISDSSI))

ggplot(dssi_data_noNA, aes(x = SIDSSI, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "SIDSSI", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()
ggplot(dssi_data_noNA, aes(x = SSDSSI, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "SSDSSI", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()

ggplot(dssi_data_noNA, aes(x = ISDSSI, fill = as.factor(cohort))) +
    geom_histogram(aes(y = ..density..), bins = 20, alpha = 0.5, position = "identity") +
    geom_density(alpha = 0.3) +
    facet_wrap(~ cohort, labeller = as_labeller(c("0" = "Control", "1" = "Case"))) +
    labs(title = "Histogram with Density Curve", x = "ISDSSI", fill = "Cohort") +
    scale_fill_discrete(labels = c("Control", "Case")) +
    theme_minimal()

#5-2) DSSI - Association
#5-2-1) SIDSSI
sidssi.nonoud <- dssi_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,SIDSSI)

sidssi.oud <- dssi_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, SIDSSI)

var.test(sidssi.nonoud$SIDSSI, sidssi.oud$SIDSSI) #p-value = 0.5777 -> equal variance -> Levene's t-test
sidssi_ttest <- t.test(sidssi.nonoud$SIDSSI, sidssi.oud$SIDSSI, var.equal=T)
print(sidssi_ttest) #statistically NOT different.

#5-2-2) SSDSSI
ssdssi.nonoud <- dssi_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,SSDSSI)

ssdssi.oud <- dssi_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, SSDSSI)

var.test(ssdssi.nonoud$SSDSSI, ssdssi.oud$SSDSSI) #p-value = 0.006159 -> unequal variance
ssdssi_ttest <- t.test(ssdssi.nonoud$SSDSSI, ssdssi.oud$SSDSSI)
print(ssdssi_ttest) #statistically different.

#5-2-3) ISDSSI
isdssi.nonoud <- dssi_data_noNA %>%
  filter(cohort==0)%>%
  select(SubjID,ISDSSI)

isdssi.oud <- dssi_data_noNA %>%
  filter(cohort==1) %>%
  select(SubjID, ISDSSI)

var.test(isdssi.nonoud$ISDSSI, isdssi.oud$ISDSSI) #p-value = 1.458e-06 -> unequal variance
isdssi_ttest <- t.test(isdssi.nonoud$ISDSSI, isdssi.oud$ISDSSI)
print(isdssi_ttest) #statistically different.

```
## 2.4.1 Descriptive Analysis
### Sample characteristics
Gender distribution is relatively even between groups. Individuals with OUD are generally younger. Marital status is higher among non-OUD individuals.Non-OUD individuals tend to have higher levels of education. Financial stability is less common among individuals with OUD.

```{r}
library(Hmisc)
library(table1)

# Preprocessing: Filter out rows where "Refused to answer" is present
final_data_clean <- final_data[!(final_data$DEM002 == "(X)Subject refused to answer" | 
                                 final_data$DEM004 == "(X)Subject refused to answer"), ]
final_data_clean <- droplevels(final_data_clean)

# Relabel the cohort variable
final_data_clean$cohort <- factor(final_data_clean$cohort, levels = c(0, 1), labels = c("Non-OUD", "OUD"))

# Assigning labels to variables for better readability
label(final_data_clean$DEM002) <- "Gender"
label(final_data_clean$DEM003) <- "Age (years)"
label(final_data_clean$DEM004) <- "Marital Status"
label(final_data_clean$DEM005) <- "Household"
label(final_data_clean$DEM006) <- "Education"
label(final_data_clean$DEM007) <- "Employment"
label(final_data_clean$DEM008) <- "Financial Status"

# Print demographic characteristics
cat("Demographic characteristics in non-OUD and OUD groups")

# Creating the table
table1(~ DEM002 + DEM003 + DEM004 + DEM005 + DEM006 + DEM007 + DEM008 | cohort, 
       data = final_data_clean, rowlabelhead = "Demographics")

```
### Predictors - descriptive analysis (not working due to missingness)
```{r}

# Relabel the cohort variable
final_data_clean$cohort <- factor(final_data_clean$cohort, levels = c(0, 1), labels = c("Non-OUD", "OUD"))

# Assigning labels to variables for better readability
label(final_data_clean$pain_severity_sum) <- "Pain severity"
label(final_data_clean$pain_interf_sum) <-  "Pain interference"
label(final_data_clean$dependence_level) <- "Nicotine dependence"
label(final_data_clean$Anxiety) <- "Anxiety"
label(final_data_clean$Depression) <- "Depression"
label(final_data_clean$ca_sum) <- "Pain Catastrophizing"
label(final_data_clean$pps_sum) <- "Mental defeat"
label(final_data_clean$suicidality_risk) <- "Suicidality"
label(final_data_clean$SIDSSI) <- "Social interaction"
label(final_data_clean$SSDSSI) <- "Satisfaction with social support"
label(final_data_clean$ISDSSI) <- "Social support"

# Print demographic characteristics
cat("Clinical and Psychosocial Characteristics Stratified by OUD Status")


```


## 2.5 Developing ML models

Given the skewness of the dataset, three algorithms were selected for their effectiveness in handling skewed variable distributions: Logistic Regression, Random Forest, and XGBoost.

### 2.5.1 Missing data Imputation

Before developing models, I need to address missing data. For this, random forest imputation was utilized. This method accounts for the distribution of variables and imputes values accordingly, rather than relying on mean, median, or mode. For continuous predictors, the imputed value is calculated as the weighted average of non-missing observations, with weights determined by proximities. For categorical predictors, the imputed value corresponds to the category with the highest average proximity. While most variables had less than 10% missing data, the mental defeat score (pps_sum) and suicidality risk exhibited significant levels of missingness. However, these variables are clinically important factors associated with the outcome (OUD). As such, they were included in the prediction model.

To evaluate whether these factors can be reliably incorporated and whether the imputation was performed appropriately, I will compare models with and without these variables. This analysis will be discussed in the section titled "Model Selection Despite Missing Data Imputation."

```{r}
library(randomForest)

##Missing data count when merged
colSums(is.na(final_data))

# Missing data imputation, using randomforest package
str(final_data)

# Using rfImpute to impute missing values in predictor variables
final_data <- final_data %>%
  mutate(across(where(is.character), as.factor))
imputed_data <- rfImpute(cohort ~ ., data = final_data, iter = 5, ntree = 500)
imputed_data <- imputed_data %>% 
  select(-SubjID)

head(imputed_data) 
colSums(is.na(imputed_data))
summary(imputed_data)

```

### 2.5.2 Missing data Imputation

```{r}
library(rsample)

set.seed (1234)
mydata_split <- initial_split(imputed_data,
                            strata = cohort,# maintaining same percentage of cases/controls
                            prop = 0.80)
mydata_training <- training(mydata_split)
mydata_test <- testing(mydata_split)

#10-fold cross validation: given there is a small sample size and I am not doing parameter tuning, I will not create held out data and use the whole dataset.
set.seed(1234)
mydata_folds <-vfold_cv(imputed_data, v=10) #okay to use the whole data, without held out data
mydata_folds
```

### 2.5.3. Model Selection With Missing Data Imputation

In order to determine whether the imputation was done properly without changing the model performance too much, I will compare two models; one with all variables included and the other with excluding these two variabels. I could also compare the model using complete cases, but given that sample size between the models too different, I chose this comparison. When comparing, along with auc roc value, false negative (which has more clinically importance) will also be considered. I will select the model that has better performance with false negative. significant predictors in two models will also be compared.

#### Model excluding the two variables with significant missingness

This model can be compared with the model including these two variables in 2.5.4.

```{r}


imputed_data_drop <- imputed_data %>%
  select(-pps_sum, -suicidality_risk)

set.seed (1234)
mydata_split_drop <- initial_split(imputed_data_drop,
                            strata = cohort,#maintaining same percentage of cases/controls
                            prop = 0.80)
mydata_training_drop <- training(mydata_split_drop)
mydata_test_drop <- testing(mydata_split_drop)

mydata_glm_drop <- glm (cohort ~., data = mydata_training_drop, family = binomial(logit))
summary(mydata_glm_drop)
vif(mydata_glm_drop)

#Model building
lr_cls_spec_2 <- 
  logistic_reg() |> 
  set_engine("glm")

#Model fit 
lr_cls_fit_2 <- 
  lr_cls_spec_2 |>
  fit(cohort ~ ., data = mydata_training_drop)

#Prediction using the "TEST" dataset
mydata.lr.pred.values.test_2 <-  bind_cols(
  truth = mydata_test_drop$cohort,
  predict(lr_cls_fit_2, mydata_test_drop),
  predict(lr_cls_fit_2, mydata_test_drop, type = "prob"))

mydata.lr.pred.values.test_2

#ROC plots for testing data
autoplot(roc_curve(mydata.lr.pred.values.test_2, 
                   truth, 
                   .pred_0))

#Metrics of prediction on test data
metrics(mydata.lr.pred.values.test_2, truth, .pred_class, .pred_0)
conf_matrix_wihtout <- mydata.lr.pred.values.test_2 %>%
  conf_mat(truth = truth, estimate = .pred_class)

# Print the confusion matrix
print(conf_matrix_wihtout)
```

Result: Given the similarity in model performance, the model that includes the two variables is preferable, as these variables hold clinical significance. More importantly, the imputation process does not appear to negatively affect the model's performance. Especially, the model with the two variables showed a slightly higher AUC-ROC value (0.88596 \< 0.88655) and fewer false negative cases (exclusion model n=29 Vs inclusion model n=28 ). Clinically, minimizing false negatives is crucial in a predictive model. Therefore, further development of the algorithm will proceed using the imputed data with these two variables included.

### 2.5.4. Logistic regression

#### glm (Training/Test)

```{r}
#general model, with the  whole dataset
forwholeglm <- final_data %>% 
  select(-SubjID)
whole_glm <- glm(cohort ~., data = forwholeglm, family=binomial(logit))
summary(whole_glm)
vif(whole_glm)

mydata_glm <- glm (cohort ~., data = mydata_training, family = binomial(logit))
summary(mydata_glm) #sig: pain_severity, smoking low, smoking non-smoker
vif(mydata_glm) #VIF all less than 5; no significant colinearity was found among variables

#Model building
lr_cls_spec <- 
  logistic_reg() |> 
  set_engine("glm")

#Model fit to training dataset
lr_cls_fit <- 
  lr_cls_spec |>
  fit(cohort ~ ., data = mydata_training)

#Prediction using the "testing" dataset
mydata.lr.pred.values.test <-  bind_cols(
  truth = mydata_test$cohort,
  predict(lr_cls_fit, mydata_test),
  predict(lr_cls_fit, mydata_test, type = "prob")
)
mydata.lr.pred.values.test

#ROC plots for "testing" dataset
autoplot(roc_curve(mydata.lr.pred.values.test, 
                   truth, 
                   .pred_0))

#Metrics of prediction on "testing" dataset
metrics(mydata.lr.pred.values.test, truth, .pred_class, .pred_0)
conf_matrix_all <- mydata.lr.pred.values.test %>%
  conf_mat(truth = truth, estimate = .pred_class)
print(conf_matrix_all)

```

#### Logistic Regression - Cross Validation (10-fold)

```{r}
#model building
lr_cls_spec_cv <- 
  logistic_reg() |> 
  set_engine("glm")

#Model fit to FULL dataset
lr_cls_fit_cv <- 
  lr_cls_spec_cv |>
  fit(cohort ~ ., data = imputed_data) #here, I'm using full data.

#Create a workflow() for fitting the glm
glm_wf <- workflow() |>
  add_model(lr_cls_spec_cv) |> 
  add_formula(cohort ~ .)
  
#Cross validation fitting
glm_fit_cv <- 
  glm_wf |>
  fit_resamples(mydata_folds, control = control_resamples(save_pred = TRUE))

#Collect predictions out of folds into one tibble
mydata_glm_cv_preds <- collect_predictions(glm_fit_cv)

#Plot of ROC curve of CV results
autoplot(roc_curve(mydata_glm_cv_preds, 
        cohort, 
        .pred_0))

#To see performance of each fold
mydata_glm_cv_preds |>
  group_by(id) |>
  roc_auc(cohort, .pred_0)

#Showing  roc curves for all folds
mydata_glm_cv_preds |>
  group_by(id) |>
  roc_curve(cohort, .pred_0) |>
  autoplot()

#Overall metrics
collect_metrics(glm_fit_cv)

```
#### Confusion matrix for lr
```{r}

# Convert probabilities to binary predictions using a threshold
mydata_glm_cv_preds <- mydata_glm_cv_preds |> 
  mutate(
    predicted = ifelse(.pred_1 > 0.5, 1, 0),               # Binary predictions
    predicted = factor(predicted, levels = c(0, 1)),       # Convert to factor
    cohort = factor(cohort, levels = c(0, 1))              # Ensure cohort is a factor
  )

# Calculate the confusion matrix
conf_mat_lr <- conf_mat(mydata_glm_cv_preds, truth = cohort, estimate = predicted)
print(conf_mat_lr)


# Accuracy
accuracy_lr <- accuracy(mydata_glm_cv_preds, truth = cohort, estimate = predicted)

# Precision
precision_lr<- precision(mydata_glm_cv_preds, truth = cohort, estimate = predicted)

# Recall (Sensitivity)
recall_lr <- recall(mydata_glm_cv_preds, truth = cohort, estimate = predicted)

# F1-Score
f1_lr <- f_meas(mydata_glm_cv_preds, truth = cohort, estimate = predicted, beta = 1)

# Display metrics
metrics_lr <- tibble(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score"),
  Value = c(accuracy_lr$.estimate, precision_lr$.estimate, recall_lr$.estimate, f1_lr$.estimate)
)
print(metrics_lr)

```


### 2.5.5. Random forest

#### Testing/Training

```{r}
library(randomForest)
library(vip)

#Model specification/building
rf_spec <- 
  rand_forest(trees = 300,min_n = 5) |> #Min number of data points in node for further split
  set_engine("randomForest", importance = TRUE) |>
  set_mode("classification")

#Model fit
rf_fit <- rf_spec |>
  fit(cohort ~ ., data = mydata_training)
rf_fit

#Prediction using the "TEST" dataset
rf_predicted <- bind_cols(
  truth = mydata_test$cohort,
  predict(rf_fit, mydata_test),
  predict(rf_fit, mydata_test, type = "prob")
)

rf_predicted

#ROC plots for testing data
roc_auc(rf_predicted, truth, .pred_0)
autoplot(roc_curve(rf_predicted, 
                   truth, 
                   .pred_0))
```

#### Random Forest Cross Validation

```{r}
#I will then use cross-validation to re-estimate the predictive ability more fairly; I will use the same folds as created for the glm model above, "mydata_folds"

#workflow setup
rf_workflow <-
  workflow() |>
  add_model(rf_spec) |>
  add_formula(cohort ~ .)

set.seed (1234)

#Use workflow to fit model with each fold of re-sampled data
rf_fit_cv <-
  rf_workflow |>
  fit_resamples(mydata_folds, control = control_resamples(save_pred = TRUE))

#one ROC plot for cross validation datasets
rf_fit_cv |>
  collect_predictions() |>
  roc_curve(cohort, .pred_0)|>
  autoplot()

# Collect predictions for each fold
cv_predictions <- rf_fit_cv |> collect_predictions()

# To see the performance of each fold using ROC AUC
cv_predictions |>
  group_by(id) |>
  roc_auc(cohort, .pred_0)

#calculating overall roc value
overall_roc_data <- cv_predictions |>
  roc_curve(cohort, .pred_0)

#Overall metrics
collect_metrics(rf_fit_cv)

```

#### Creating 10-fold CV ROC curves for visualization
```{r}
# Calculate ROC data for each fold
fold_roc_data <- cv_predictions |>
  group_by(id) |>
  roc_curve(cohort, .pred_0)

# Calculate AUC for each fold
fold_auc <- cv_predictions |>
  group_by(id) |>
  roc_auc(cohort, .pred_0)
fold_auc

# Calculate the overall ROC curve
overall_roc_data <- cv_predictions |>
  roc_curve(cohort, .pred_0)


# Create labels with AUC for each fold
fold_auc_labels <- fold_auc |>
  mutate(label = paste0(id, " (AUC = ", round(.estimate, 3), ")"))

# Update the fold IDs in cv_predictions with the new labels
cv_predictions <- cv_predictions |>
  left_join(fold_auc_labels, by = "id") |>
  mutate(id = label)

# Plot ROC curves for each fold with AUC in legend and overlay overall ROC curve
cv_predictions |>
  group_by(id) |>
  roc_curve(cohort, .pred_0) |>
  autoplot() +
  geom_line(data = overall_roc_data, aes(x = 1 - specificity, y = sensitivity), 
            color = "black", size = 0.5) +
  labs(
    title = "ROC Curves for Each Fold with Overall ROC Curve",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Fold (AUC)"
  ) +
  theme_minimal()



#To understand the variables that most contribute tot eh classification, I extract the importance scores using the vip package.
rf_fit |>
  extract_fit_engine() |>
  importance()

rf_fit |>
  extract_fit_engine() |>
  vip()
```
#### Confusion matrix/ calculation
```{r}

# Ensure that both 'cohort' (truth) and 'predicted' (estimate) are factors
cv_predictions <- cv_predictions |> 
  mutate(predicted = ifelse(.pred_1 > 0.5, 1, 0))

cv_predictions <- cv_predictions |>
  mutate(
    predicted = factor(predicted, levels = c(0, 1)),  # Convert predicted to a factor
    cohort = factor(cohort, levels = c(0, 1))         # Convert cohort to a factor
  )
# Confusion matrix
conf_mat <- cv_predictions |> 
  conf_mat(truth = cohort, estimate = predicted)
print(conf_mat)

# Accuracy
accuracy <- accuracy(cv_predictions, truth = cohort, estimate = predicted)

# Precision
precision <- precision(cv_predictions, truth = cohort, estimate = predicted)

# Recall (Sensitivity)
recall <- recall(cv_predictions, truth = cohort, estimate = predicted)

# F1-Score
f1 <- f_meas(cv_predictions, truth = cohort, estimate = predicted, beta = 1)

# Display metrics
metrics <- tibble(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score"),
  Value = c(accuracy$.estimate, precision$.estimate, recall$.estimate, f1$.estimate)
)
print(metrics)

```


### 2.5.6 Gradient boosting

#### Testing/Training

```{r}
#loading the packages
library(xgboost)

#Model specification/building
bt_spec <- 
  boost_tree(trees = 50,        #Number of trees
             tree_depth = 4) |> #Max depth of tree
  set_mode("classification") |>
  set_engine("xgboost")
bt_spec

#Recipe
bt_recipe <-
  recipe(cohort ~ ., data = mydata_training) |>
  step_dummy(all_nominal_predictors()) |>
  step_normalize(all_predictors())

#Workflow specification
bt_workflow <- workflow() |>
  add_model(bt_spec) |>
  add_recipe(bt_recipe)

#Model fit with training data
bt_fit <- fit(bt_workflow, data = mydata_training)
bt_fit

#Prediction model using the "TEST" dataset
my.bt.pred <- bind_cols(
  truth = mydata_test$cohort,
  predict(bt_fit, mydata_test),
  predict(bt_fit, mydata_test, type = "prob"))

#ROC plots for testing data
roc_auc(my.bt.pred, truth, 
        .pred_0)
autoplot(roc_curve(my.bt.pred, 
                   truth, 
                   .pred_0))

#Variable importance extracted with vip
vip(bt_fit)

# Variable importance extracted from xgboost object
bt_object <- pull_workflow_fit(bt_fit)$fit
xgb.importance(model = bt_object)
```

#### Cross Validation - Testing/Training

```{r}
#workflow setup
bt_workflow <-
  workflow() |>
  add_model(bt_spec) |>
  add_formula(cohort ~ .)

set.seed (1234)
#Use workflow to fit model with each fold of re-sampled data
bt_fit_cv <-
  bt_workflow |>
  fit_resamples(mydata_folds, control = control_resamples(save_pred = TRUE))

#ROC plots for cross validation datasets
bt_fit_cv |>
  collect_predictions() |>
  roc_curve(cohort, .pred_0)|>
  autoplot()

#Overall metrics
collect_metrics(bt_fit_cv)

```

#### Confusion matrix
```{r}

# Collect predictions from the cross-validation results
bt_predictions <- bt_fit_cv |>
  collect_predictions()

# Convert probabilities to binary predictions
bt_predictions <- bt_predictions |> 
  mutate(
    predicted = ifelse(.pred_1 > 0.5, 1, 0),               # Binary predictions
    predicted = factor(predicted, levels = c(0, 1)),       # Convert to factor
    cohort = factor(cohort, levels = c(0, 1))              # Ensure cohort is a factor
  )

# Calculate overall metrics
bt_metrics <- bt_predictions |>
  metrics(truth = cohort, estimate = predicted, .pred_1) |>
  filter(.metric %in% c("accuracy", "precision", "recall", "f_meas"))

print(bt_metrics)

# Accuracy
accuracy_bt <- accuracy(bt_predictions, truth = cohort, estimate = predicted)

# Precision
precision_bt<- precision(bt_predictions, truth = cohort, estimate = predicted)

# Recall (Sensitivity)
recall_bt<- recall(bt_predictions, truth = cohort, estimate = predicted)

# F1-Score
f1_bt <- f_meas(bt_predictions, truth = cohort, estimate = predicted, beta = 1)

# Display metrics
metrics_bt <- tibble(
  Metric = c("Accuracy", "Precision", "Recall", "F1-Score"),
  Value = c(accuracy_bt$.estimate, precision_bt$.estimate, recall_bt$.estimate, f1_bt$.estimate)
)
print(metrics_bt)

```


### 2.5.7.Model Performance

Three predictive models were implemented to predict opioid use disorder. 10-fold cross validation was conducted to evaluate model performances. All predictive models achieved satisfactory performance When comparing roc_auc values for those 3 algoriths with cross-validation, ramdom forests had the highest value.

```{r}
#Naming
lr_roc_data <- roc_curve(mydata.lr.pred.values.test, truth, .pred_0)
lr_roc_cv_data <- roc_curve(mydata_glm_cv_preds, cohort, .pred_0)
rf_roc_data <- roc_curve (rf_predicted, truth,.pred_0)
rf_roc_cv_data <- rf_fit_cv |>
  collect_predictions() |>
  roc_curve(cohort, .pred_0)
bt_roc_data <- roc_curve (my.bt.pred, truth, .pred_0)
bt_roc_data <- bt_fit_cv |>
  collect_predictions() |>
  roc_curve(cohort, .pred_0)

#ggplot
ggplot() +
  geom_path(data = lr_roc_data, aes(x = 1 - specificity, 
                                     y = sensitivity, 
                                     color = "blue")) +
  geom_path(data = lr_roc_cv_data, aes(x = 1 - specificity, 
                                        y = sensitivity,
                                        color = "lightblue")) +
  geom_path(data = rf_roc_data, aes(x = 1 - specificity, 
                                       y = sensitivity,
                                       color = "pink")) +
  geom_path(data = rf_roc_cv_data, aes(x = 1 - specificity, 
                                    y = sensitivity, 
                                    color = "red")) +
  geom_path(data = bt_roc_data, aes(x = 1 - specificity, 
                                    y = sensitivity, 
                                    color = "purple")) +
   geom_path(data = bt_roc_data, aes(x = 1 - specificity, 
                                    y = sensitivity, 
                                    color = "Green")) +
  labs(
    title = "Combined Cross-Validated ROC Curves with Confidence Intervals",
    x = "False Positive Rate",
    y = "True Positive Rate",
    color = "Model",
    fill = "Model"
  ) +
  scale_color_manual(values = c("blue", "lightblue", "pink", "red", "purple", "Green"), 
                     labels = c("LR",
                                "LR - 10-fold CV",
                                "RF",
                                "RF - 10-fold CV",
                                "GT",
                                "GT - 10-fold CV")) +  
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()


```

# Discussion

The RF model demonstrated the highest overall performance, with an F1-score of 0.8936, a recall of 0.9276, and the highest AUC of 0.8969, indicating superior discriminatory ability. The XGBoost model achieved the highest precision (0.8669) but had a slightly lower AUC (0.8780) compared to the other models. The 10-fold ROC curves (light blue for LR, red for RF, and green for GT) highlight the models' performance across different subsets of the data. While RF shows the best overall sensitivity across a range of false positive rates, GT underperformed relative to RF and LR, as evidenced by its lower AUC despite visually appearing to follow RF's trajectory. LR, although slightly less sensitive than RF in some regions, exhibited more consistent performance than GT. These findings indicate that RF offers the best overall predictive performance, while GT may struggle with reliability compared to LR and RF in this specific task. 

In the 10-fold models of the random forest algorithm, the AUC values for the folds range from 0.869 to 0.927, indicating consistently strong performance across all folds, with minor variability. And they are closely clustered around the overalll curve, which suggest the model's stability. 

This model has clinical and research implications. In Practice, this model enables to identify high-risk patients early, which can help clinical decision making. Also, feature importance analyss identified top three factors that contribute to the outcome, therefore, interventions need to be developed targeting these factors. Interestingly, these results align with certain findings in genetics, particularly the shared genetic risk factors between nicotine and OUD. Also, this study highlights the role of mental defeat — a factor not yet incorporated into existing OUD models — which should be explored further in future research.This work also lays the foundation for future research by providing a strong predictive framework to support the development of more advanced machine learning models.


## Limitation

The results of these analysis need to be interpreted with caution. Importantly, due to the genotypic objective of the parent study, all participants were Caucasian participants, limiting the generalizability of findings to more diverse populations. Secondly, significant number of missing data were observed in two key factors within this model. Despite this, these variables are clinically considered highly relevant to the outcome, and their imputation does not appear to have adversely affected the model's performance, as discussed previously. Lastly, this model was developed based on a relatively small sample size. However, it was specifically tailored for chronic pain patients on long-term opioid therapy, a population at higher risk of developing OUD compared to the general population. Considering that the model incorporates various significant risk factors that are often excluded from existing machine learning models, this model offers meaningful insights and potential contributions to the field.


# Conclusion

Random forest achieved the best predictive performance, with an AUC-ROC of 0.89. Feature importance analysis identified nicotine dependence, age, and mental defeat as the top three significant features.This model examined the data performance using the pre-determined variables. This model showed satisfactory prediction performance, and it can be a useful tool for clinicians to identify patients at risk for developing OUD and implement early intervention for prevention. This model not only offers a strong predictive framework, but also lays the groundwork for more advanced machine learning models that make use of a broader range of variables, ultimately improving early identification and intervention strategies for at-risk patients. 

